{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386bd1ca-7524-4dd9-b84e-09270f5de510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type not provided, defaulting to radgraph-xl\n",
      "['acute', 'cardiopulmonary', 'process', 'moderate', 'hiatal', 'hernia']\n"
     ]
    }
   ],
   "source": [
    "def extract_tokens_from_radgraph(text_list):\n",
    "    from radgraph import RadGraph\n",
    "    \n",
    "    # Initialize RadGraph\n",
    "    radgraph = RadGraph()\n",
    "    \n",
    "    # Process the input text\n",
    "    annotations = radgraph(text_list)\n",
    "    \n",
    "    # Extract only the tokens from each entity\n",
    "    all_tokens = []\n",
    "    \n",
    "    for doc_id, doc_data in annotations.items():\n",
    "        for entity_id, entity_data in doc_data['entities'].items():\n",
    "            # Some tokens might be phrases (multi-word), so split if needed\n",
    "            tokens = entity_data['tokens'].split()\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "    return all_tokens\n",
    "\n",
    "# Example usage\n",
    "text = [\"no evidence of acute cardiopulmonary process moderate hiatal hernia\"]\n",
    "token_list = extract_tokens_from_radgraph(text)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349cd682-4295-4724-80a0-b4ceed4ff38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "顶级键： dict_keys(['train', 'val', 'test'])\n",
      "Train 样本数量： 270790\n",
      "数据集概览： DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'study_id', 'subject_id', 'report', 'image_path', 'split', 'label'],\n",
      "        num_rows: 270790\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['id', 'study_id', 'subject_id', 'report', 'image_path', 'split', 'label'],\n",
      "        num_rows: 2130\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'study_id', 'subject_id', 'report', 'image_path', 'split', 'label'],\n",
      "        num_rows: 3858\n",
      "    })\n",
      "})\n",
      "样本 1: {'id': '02aa804e-bde0afdd-112c0b34-7bc16630-4e384014', 'study_id': 50414267, 'subject_id': 10000032, 'report': 'There is no focal consolidation, pleural effusion or pneumothorax.  Bilateral\\n nodular opacities that most likely represent nipple shadows. The\\n cardiomediastinal silhouette is normal.  Clips project over the left lung,\\n potentially within the breast. The imaged upper abdomen is unremarkable.\\n Chronic deformity of the posterior left sixth and seventh ribs are noted.', 'image_path': ['p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg'], 'split': 'train', 'label': [False, False, False, True, True, False, False, False, False, False, False, False, False, False]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/home/ghan/R2Gen/data/mimic_cxr/annotation_label.json\"\n",
    "\n",
    "# 加载 JSON 文件\n",
    "with open(file_path, 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# 检查顶级键\n",
    "print(\"顶级键：\", raw_data.keys())\n",
    "\n",
    "# 检查 train 的样本数量\n",
    "print(\"Train 样本数量：\", len(raw_data['train']))\n",
    "\n",
    "# 将数据转换为 DatasetDict\n",
    "dataset_dict = {\n",
    "    split: Dataset.from_list(raw_data[split]) for split in raw_data.keys()\n",
    "}\n",
    "dataset = DatasetDict(dataset_dict)\n",
    "\n",
    "# 打印概览\n",
    "print(\"数据集概览：\", dataset)\n",
    "\n",
    "for i in range(min(1, len(dataset['train']))):\n",
    "    print(f\"样本 {i+1}: {dataset['train'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28722ef3-afe0-493d-9504-4fb401fd92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Processing reports with RadGraph using 8 threads...\n",
      "model_type not provided, defaulting to radgraph-xl\n",
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 270790/270790 [5:58:58<00:00, 12.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2130/2130 [01:25<00:00, 25.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3858/3858 [02:51<00:00, 22.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to /home/ghan/R2Gen/data/mimic_cxr/annotation_label_with_tokens.json...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from radgraph import RadGraph\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_report(report_text, radgraph_model):\n",
    "    \"\"\"Process a single report with RadGraph and extract tokens, splitting multi-word tokens\"\"\"\n",
    "    annotations = radgraph_model([report_text])\n",
    "    \n",
    "    # Extract tokens and split multi-word tokens\n",
    "    tokens = []\n",
    "    if annotations and '0' in annotations:\n",
    "        for entity_id, entity_data in annotations['0']['entities'].items():\n",
    "            # Split multi-word tokens by space\n",
    "            token_text = entity_data['tokens']\n",
    "            individual_tokens = token_text.split()\n",
    "            tokens.extend(individual_tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def process_dataset_with_radgraph(dataset_dict, num_workers=8):\n",
    "    # Initialize RadGraph\n",
    "    radgraph = RadGraph()\n",
    "    \n",
    "    # Create a new dictionary to store the processed data\n",
    "    processed_data = {}\n",
    "    \n",
    "    # Process each split\n",
    "    for split_name, split_dataset in dataset_dict.items():\n",
    "        print(f\"Processing {split_name} split...\")\n",
    "        processed_split = []\n",
    "        all_samples = list(split_dataset)\n",
    "        \n",
    "        # Create a progress bar\n",
    "        pbar = tqdm(total=len(all_samples), desc=f\"Processing {split_name}\")\n",
    "        \n",
    "        # Setup ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            # Submit tasks\n",
    "            future_to_sample = {}\n",
    "            for sample in all_samples:\n",
    "                future = executor.submit(process_report, sample['report'], radgraph)\n",
    "                future_to_sample[future] = sample\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in as_completed(future_to_sample):\n",
    "                sample = future_to_sample[future]\n",
    "                try:\n",
    "                    tokens = future.result()\n",
    "                    new_sample = sample.copy()\n",
    "                    new_sample['tokens'] = tokens\n",
    "                    processed_split.append(new_sample)\n",
    "                except Exception as exc:\n",
    "                    print(f\"Sample processing generated an exception: {exc}\")\n",
    "                    # Still add the sample but with empty tokens\n",
    "                    new_sample = sample.copy()\n",
    "                    new_sample['tokens'] = []\n",
    "                    processed_split.append(new_sample)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Close progress bar\n",
    "        pbar.close()\n",
    "        processed_data[split_name] = processed_split\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    file_path = \"/home/ghan/R2Gen/data/mimic_cxr/annotation_label.json\"\n",
    "    output_path = \"/home/ghan/R2Gen/data/mimic_cxr/annotation_label_with_tokens.json\"\n",
    "    \n",
    "    # Number of worker threads\n",
    "    num_workers = 8  # Adjust based on your CPU cores\n",
    "    \n",
    "    # Load JSON file\n",
    "    print(\"Loading dataset...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Convert to DatasetDict\n",
    "    dataset_dict = {\n",
    "        split: Dataset.from_list(raw_data[split]) for split in raw_data.keys()\n",
    "    }\n",
    "    dataset = DatasetDict(dataset_dict)\n",
    "    \n",
    "    # Process with RadGraph using multiple threads\n",
    "    print(f\"Processing reports with RadGraph using {num_workers} threads...\")\n",
    "    processed_data = process_dataset_with_radgraph(dataset, num_workers)\n",
    "    \n",
    "    # Save processed data to new JSON file\n",
    "    print(f\"Saving processed data to {output_path}...\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d99a6cba-c69f-4421-9880-5ebdf61d8c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Processing reports with RadGraph using 8 threads...\n",
      "model_type not provided, defaulting to radgraph-xl\n",
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2069/2069 [01:08<00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 296/296 [00:09<00:00, 30.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 590/590 [00:18<00:00, 31.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to /home/ghan/R2Gen/data/iu_xray/annotation_label_with_tokens.json...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from radgraph import RadGraph\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_report(report_text, radgraph_model):\n",
    "    \"\"\"Process a single report with RadGraph and extract tokens, splitting multi-word tokens\"\"\"\n",
    "    annotations = radgraph_model([report_text])\n",
    "    \n",
    "    # Extract tokens and split multi-word tokens\n",
    "    tokens = []\n",
    "    if annotations and '0' in annotations:\n",
    "        for entity_id, entity_data in annotations['0']['entities'].items():\n",
    "            # Split multi-word tokens by space\n",
    "            token_text = entity_data['tokens']\n",
    "            individual_tokens = token_text.split()\n",
    "            tokens.extend(individual_tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def process_dataset_with_radgraph(dataset_dict, num_workers=8):\n",
    "    # Initialize RadGraph\n",
    "    radgraph = RadGraph()\n",
    "    \n",
    "    # Create a new dictionary to store the processed data\n",
    "    processed_data = {}\n",
    "    \n",
    "    # Process each split\n",
    "    for split_name, split_dataset in dataset_dict.items():\n",
    "        print(f\"Processing {split_name} split...\")\n",
    "        processed_split = []\n",
    "        all_samples = list(split_dataset)\n",
    "        \n",
    "        # Create a progress bar\n",
    "        pbar = tqdm(total=len(all_samples), desc=f\"Processing {split_name}\")\n",
    "        \n",
    "        # Setup ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            # Submit tasks\n",
    "            future_to_sample = {}\n",
    "            for sample in all_samples:\n",
    "                future = executor.submit(process_report, sample['report'], radgraph)\n",
    "                future_to_sample[future] = sample\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in as_completed(future_to_sample):\n",
    "                sample = future_to_sample[future]\n",
    "                try:\n",
    "                    tokens = future.result()\n",
    "                    new_sample = sample.copy()\n",
    "                    new_sample['tokens'] = tokens\n",
    "                    processed_split.append(new_sample)\n",
    "                except Exception as exc:\n",
    "                    print(f\"Sample processing generated an exception: {exc}\")\n",
    "                    # Still add the sample but with empty tokens\n",
    "                    new_sample = sample.copy()\n",
    "                    new_sample['tokens'] = []\n",
    "                    processed_split.append(new_sample)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Close progress bar\n",
    "        pbar.close()\n",
    "        processed_data[split_name] = processed_split\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    file_path = \"/home/ghan/R2Gen/data/iu_xray/annotation_label.json\"\n",
    "    output_path = \"/home/ghan/R2Gen/data/iu_xray/annotation_label_with_tokens.json\"\n",
    "    \n",
    "    # Number of worker threads\n",
    "    num_workers = 8  # Adjust based on your CPU cores\n",
    "    \n",
    "    # Load JSON file\n",
    "    print(\"Loading dataset...\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    # Convert to DatasetDict\n",
    "    dataset_dict = {\n",
    "        split: Dataset.from_list(raw_data[split]) for split in raw_data.keys()\n",
    "    }\n",
    "    dataset = DatasetDict(dataset_dict)\n",
    "    \n",
    "    # Process with RadGraph using multiple threads\n",
    "    print(f\"Processing reports with RadGraph using {num_workers} threads...\")\n",
    "    processed_data = process_dataset_with_radgraph(dataset, num_workers)\n",
    "    \n",
    "    # Save processed data to new JSON file\n",
    "    print(f\"Saving processed data to {output_path}...\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024d734-b659-4764-8881-fb31c69364c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
