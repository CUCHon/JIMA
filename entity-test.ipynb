{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e400c6db-e18d-4465-8f23-6df3257d3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "import argparse\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __init__(self, args):\n",
    "        self.ann_path = args.ann_path\n",
    "        self.threshold = args.threshold\n",
    "        self.dataset_name = args.dataset_name\n",
    "        if self.dataset_name == 'iu_xray':\n",
    "            self.clean_report = self.clean_report_iu_xray\n",
    "        else:\n",
    "            self.clean_report = self.clean_report_mimic_cxr\n",
    "        self.ann = json.loads(open(self.ann_path, 'r').read())\n",
    "        self.token2idx, self.idx2token = self.create_vocabulary()\n",
    "\n",
    "    def create_vocabulary(self):\n",
    "        total_tokens = []\n",
    "\n",
    "        for example in self.ann['train']:\n",
    "            tokens = self.clean_report(example['report']).split()\n",
    "            for token in tokens:\n",
    "                total_tokens.append(token)\n",
    "\n",
    "        counter = Counter(total_tokens)\n",
    "        vocab = [k for k, v in counter.items() if v >= self.threshold] + ['<unk>']\n",
    "        vocab.sort()\n",
    "        token2idx, idx2token = {}, {}\n",
    "        for idx, token in enumerate(vocab):\n",
    "            token2idx[token] = idx + 1\n",
    "            idx2token[idx + 1] = token\n",
    "        return token2idx, idx2token\n",
    "\n",
    "    def clean_report_iu_xray(self, report):\n",
    "        report_cleaner = lambda t: t.replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '') \\\n",
    "            .replace('. 2. ', '. ').replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ') \\\n",
    "            .replace(' 2. ', '. ').replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n",
    "            .strip().lower().split('. ')\n",
    "        sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '').\n",
    "                                        replace('\\\\', '').replace(\"'\", '').strip().lower())\n",
    "        tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != []]\n",
    "        report = ' . '.join(tokens) + ' .'\n",
    "        return report\n",
    "\n",
    "    def clean_report_mimic_cxr(self, report):\n",
    "        report_cleaner = lambda t: t.replace('\\n', ' ').replace('__', '_').replace('__', '_').replace('__', '_') \\\n",
    "            .replace('__', '_').replace('__', '_').replace('__', '_').replace('__', '_').replace('  ', ' ') \\\n",
    "            .replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ').replace('  ', ' ') \\\n",
    "            .replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.').replace('..', '.') \\\n",
    "            .replace('..', '.').replace('..', '.').replace('..', '.').replace('1. ', '').replace('. 2. ', '. ') \\\n",
    "            .replace('. 3. ', '. ').replace('. 4. ', '. ').replace('. 5. ', '. ').replace(' 2. ', '. ') \\\n",
    "            .replace(' 3. ', '. ').replace(' 4. ', '. ').replace(' 5. ', '. ') \\\n",
    "            .strip().lower().split('. ')\n",
    "        sent_cleaner = lambda t: re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '')\n",
    "                                        .replace('\\\\', '').replace(\"'\", '').strip().lower())\n",
    "        tokens = [sent_cleaner(sent) for sent in report_cleaner(report) if sent_cleaner(sent) != []]\n",
    "        report = ' . '.join(tokens) + ' .'\n",
    "        return report\n",
    "\n",
    "    def get_token_by_id(self, id):\n",
    "        return self.idx2token[id]\n",
    "\n",
    "    def get_id_by_token(self, token):\n",
    "        if token not in self.token2idx:\n",
    "            return self.token2idx['<unk>']\n",
    "        return self.token2idx[token]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.token2idx)\n",
    "\n",
    "    def __call__(self, report):\n",
    "        tokens = self.clean_report(report).split()\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.get_id_by_token(token))\n",
    "        ids = [0] + ids + [0]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        txt = ''\n",
    "        for i, idx in enumerate(ids):\n",
    "            if idx > 0:\n",
    "                if i >= 1:\n",
    "                    txt += ' '\n",
    "                txt += self.idx2token[idx]\n",
    "            else:\n",
    "                break\n",
    "        return txt\n",
    "\n",
    "    def decode_batch(self, ids_batch):\n",
    "        out = []\n",
    "        for ids in ids_batch:\n",
    "            out.append(self.decode(ids))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f8ca649-4588-47b8-a135-5921dee29574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 使用一个简单的参数对象来替代argparse的解析\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.ann_path = 'data/iu_xray/annotation_label_with_tokens.json'\n",
    "            self.threshold = 3\n",
    "            self.dataset_name = 'iu_xray'\n",
    "            self.max_seq_length = 200\n",
    "            self.seed = 9233\n",
    "            # 添加其他必要的参数\n",
    "    \n",
    "    return Args()\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 主函数\n",
    "def init_tokenizer():\n",
    "    # 获取参数\n",
    "    args = get_args()\n",
    "    \n",
    "    # 设置随机种子\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    # 创建tokenizer\n",
    "    tokenizer = Tokenizer(args)\n",
    "    return tokenizer\n",
    "\n",
    "# 在Jupyter中运行此代码以获取tokenizer\n",
    "tokenizer = init_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20ab8465-fcaf-4ac6-97c9-51267d4b6097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tokenizer at 0x7df91d6ae390>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ce14fe7-08d6-4ed8-b745-45087a83bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 760\n"
     ]
    }
   ],
   "source": [
    "# 输出词汇表大小\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05e1b67c-700d-4c2e-92b7-3039a7d55fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data...\n",
      "Collecting tokens from processed data...\n",
      "Processing train split...\n",
      "Processing val split...\n",
      "Processing test split...\n",
      "Checking tokens against tokenizer vocabulary...\n",
      "\n",
      "===== Token Analysis =====\n",
      "Total unique tokens from RadGraph (lowercase): 1141\n",
      "Tokens present in tokenizer: 595 (52.15%)\n",
      "Tokens missing from tokenizer: 546 (47.85%)\n",
      "\n",
      "===== Most Frequent Missing Tokens =====\n",
      "'-' - appears 174 times\n",
      "Example context: 'The cardiomediastinal silhouette is normal in size and contour. There are a few XXXX opacities in th...'\n",
      "--------------------------------------------------\n",
      "'sided' - appears 58 times\n",
      "Example context: 'The heart size is moderate to severely enlarged. There is prominence of the central pulmonary XXXX s...'\n",
      "--------------------------------------------------\n",
      "'t' - appears 28 times\n",
      "Example context: 'The aortic XXXX is mildly tortuous. The cardiomediastinal silhouette and pulmonary vasculature are w...'\n",
      "--------------------------------------------------\n",
      "'indeterminate' - appears 11 times\n",
      "Example context: 'Elevated right hemidiaphragm. Clear lungs. No pleural effusions or pneumothoraces. heart size is upp...'\n",
      "--------------------------------------------------\n",
      "'defined' - appears 8 times\n",
      "Example context: 'The heart size size and pulmonary vascularity appear within normal limits. Ill-defined opacity is ag...'\n",
      "--------------------------------------------------\n",
      "'0' - appears 8 times\n",
      "Example context: 'there is a rounded opacity in the right lower zone measuring 2.0 cm which is XXXX to be in the poste...'\n",
      "--------------------------------------------------\n",
      "'shaped' - appears 7 times\n",
      "Example context: 'Calcified granulomas noted. XXXX symmetric apical scarring. The diaphragms are flattened, and the ch...'\n",
      "--------------------------------------------------\n",
      "'pneumoperitoneum' - appears 7 times\n",
      "Example context: 'Rounded 1.4 cm projecting retrosternally on lateral view only. No focal consolidation, effusion, or ...'\n",
      "--------------------------------------------------\n",
      "'4' - appears 7 times\n",
      "Example context: 'No pneumothorax, pleural effusion, or focal airspace disease. There is a discrete 1.4 cm nodule with...'\n",
      "--------------------------------------------------\n",
      "'ill' - appears 7 times\n",
      "Example context: 'The heart size size and pulmonary vascularity appear within normal limits. Ill-defined opacity is ag...'\n",
      "--------------------------------------------------\n",
      "'device' - appears 6 times\n",
      "Example context: 'There is a stable closure device projected over the heart. The heart and mediastinum are otherwise n...'\n",
      "--------------------------------------------------\n",
      "'colon' - appears 6 times\n",
      "Example context: 'There are broken 1st and 3rd-5XXXX XXXX XXXX. Normal cardiomediastinal silhouette. Pulmonary vascula...'\n",
      "--------------------------------------------------\n",
      "'dual' - appears 6 times\n",
      "Example context: 'There is a dual-lumen right internal jugular central venous catheter, the distal tip projects over t...'\n",
      "--------------------------------------------------\n",
      "'lumen' - appears 6 times\n",
      "Example context: 'There is a dual-lumen right internal jugular central venous catheter, the distal tip projects over t...'\n",
      "--------------------------------------------------\n",
      "'basal' - appears 5 times\n",
      "Example context: 'XXXX onset right basal atelectasis with airspace disease and effusion suggestive of the chest infect...'\n",
      "--------------------------------------------------\n",
      "'consolidative' - appears 5 times\n",
      "Example context: 'Postsurgical changes are noted in the mediastinum. There is tortuosity and/or ectasia of the thoraci...'\n",
      "--------------------------------------------------\n",
      "'convexity' - appears 5 times\n",
      "Example context: 'Heart size is XXXX within normal limits. There are surgical clips in the left mediastinum. There is ...'\n",
      "--------------------------------------------------\n",
      "'bullous' - appears 5 times\n",
      "Example context: 'There is stable left costophrenic XXXX blunting. The patient has undergone prior left lobectomy. The...'\n",
      "--------------------------------------------------\n",
      "'/' - appears 5 times\n",
      "Example context: 'The lungs are clear bilaterally. Specifically, no evidence of focal consolidation, pneumothorax, or ...'\n",
      "--------------------------------------------------\n",
      "'9' - appears 5 times\n",
      "Example context: 'Normal heart size and mediastinal contours. The lungs are free of any focal airspace disease. In the...'\n",
      "--------------------------------------------------\n",
      "\n",
      "Tokens only in tokenizer (not in RadGraph): 165\n",
      "Examples: ['radiograph', 'for', 'concerning', 'would', 'consider', 'due', 'involving', 'rotation', 'any', 'previously']\n",
      "\n",
      "Saving analysis results to /home/ghan/R2Gen/data/iu_xray/token_analysis.json\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 假设你已经初始化了tokenizer\n",
    "# tokenizer = init_tokenizer()  # 如果还没有初始化，请先运行之前的代码\n",
    "\n",
    "# 1. 读取处理后的文件\n",
    "processed_file_path = \"/home/ghan/R2Gen/data/iu_xray/annotation_label_with_tokens.json\"\n",
    "\n",
    "print(\"Loading processed data...\")\n",
    "with open(processed_file_path, 'r') as f:\n",
    "    processed_data = json.load(f)\n",
    "\n",
    "# 2. 收集所有的tokens并去重（确保全部小写）\n",
    "all_tokens = set()\n",
    "token_counter = Counter()\n",
    "token_examples = {}  # 保存每个token出现的一个示例\n",
    "\n",
    "print(\"Collecting tokens from processed data...\")\n",
    "for split_name, split_data in processed_data.items():\n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    for sample in split_data:\n",
    "        if 'tokens' in sample:\n",
    "            # 确保所有token都转为小写\n",
    "            lowercase_tokens = [token.lower() for token in sample['tokens']]\n",
    "            \n",
    "            # 记录每个token的出现次数\n",
    "            for token in lowercase_tokens:\n",
    "                token_counter[token] += 1\n",
    "                \n",
    "                # 如果还没有这个token的示例，保存当前样本作为示例\n",
    "                if token not in token_examples and 'report' in sample:\n",
    "                    token_examples[token] = sample['report']\n",
    "            \n",
    "            # 将所有小写token添加到集合中去重\n",
    "            all_tokens.update(lowercase_tokens)\n",
    "\n",
    "# 3. 检查是否都在tokenizer的词汇表中\n",
    "missing_tokens = []\n",
    "existing_tokens = []\n",
    "\n",
    "print(\"Checking tokens against tokenizer vocabulary...\")\n",
    "for token in all_tokens:\n",
    "    if token in tokenizer.token2idx:\n",
    "        existing_tokens.append(token)\n",
    "    else:\n",
    "        missing_tokens.append(token)\n",
    "\n",
    "# 4. 统计和输出结果\n",
    "print(\"\\n===== Token Analysis =====\")\n",
    "print(f\"Total unique tokens from RadGraph (lowercase): {len(all_tokens)}\")\n",
    "print(f\"Tokens present in tokenizer: {len(existing_tokens)} ({len(existing_tokens)/len(all_tokens)*100:.2f}%)\")\n",
    "print(f\"Tokens missing from tokenizer: {len(missing_tokens)} ({len(missing_tokens)/len(all_tokens)*100:.2f}%)\")\n",
    "\n",
    "# 5. 展示一些最频繁的缺失token\n",
    "if missing_tokens:\n",
    "    print(\"\\n===== Most Frequent Missing Tokens =====\")\n",
    "    most_frequent_missing = [(token, token_counter[token]) for token in missing_tokens]\n",
    "    most_frequent_missing.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 显示前20个最频繁缺失的token和它们的示例\n",
    "    for token, count in most_frequent_missing[:20]:\n",
    "        example = token_examples.get(token, \"N/A\")\n",
    "        print(f\"'{token}' - appears {count} times\")\n",
    "        print(f\"Example context: '{example[:100]}...'\")  # 只显示前100个字符\n",
    "        print(\"-\"*50)\n",
    "\n",
    "# 6. 展示tokenizer中有但RadGraph没提取的tokens\n",
    "tokenizer_only_tokens = set(tokenizer.token2idx.keys()) - all_tokens\n",
    "if len(tokenizer_only_tokens) > 0:\n",
    "    print(f\"\\nTokens only in tokenizer (not in RadGraph): {len(tokenizer_only_tokens)}\")\n",
    "    print(\"Examples:\", list(tokenizer_only_tokens)[:10])  # 显示10个示例\n",
    "\n",
    "# 7. 保存分析结果以供进一步研究\n",
    "output_analysis_path = \"/home/ghan/R2Gen/data/iu_xray/token_analysis.json\"\n",
    "analysis_results = {\n",
    "    \"total_radgraph_tokens\": len(all_tokens),\n",
    "    \"tokens_in_tokenizer\": len(existing_tokens),\n",
    "    \"tokens_missing_from_tokenizer\": len(missing_tokens),\n",
    "    \"missing_tokens_with_frequency\": dict([(token, token_counter[token]) for token in missing_tokens]),\n",
    "    \"tokenizer_only_tokens\": list(tokenizer_only_tokens)\n",
    "}\n",
    "\n",
    "print(f\"\\nSaving analysis results to {output_analysis_path}\")\n",
    "with open(output_analysis_path, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d30a72a-df4f-469f-9968-60c2507979a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/ghan/R2Gen/data/iu_xray/annotation_label_with_tokens.json...\n",
      "Processing tokens...\n",
      "Processing train split with 2069 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val split with 296 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split with 590 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Filtering Results =====\n",
      "Total tokens before filtering: 51709\n",
      "Total tokens after filtering: 50572\n",
      "Removed tokens: 1137 (2.20% of original)\n",
      "Samples that lost all their tokens: 0\n",
      "Saving filtered data to /home/ghan/R2Gen/data/iu_xray/annotation_label_with_filtered_tokens.json...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设你已经初始化了tokenizer\n",
    "# tokenizer = init_tokenizer()  # 如果还没有初始化，请先运行之前的代码\n",
    "\n",
    "def filter_tokens_by_tokenizer(input_path, output_path, tokenizer):\n",
    "    print(f\"Loading data from {input_path}...\")\n",
    "    with open(input_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 统计信息\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    removed_tokens_count = 0\n",
    "    samples_with_no_tokens = 0\n",
    "    \n",
    "    print(\"Processing tokens...\")\n",
    "    # 处理每个分割\n",
    "    for split_name, split_data in data.items():\n",
    "        print(f\"Processing {split_name} split with {len(split_data)} samples...\")\n",
    "        \n",
    "        # 使用tqdm创建进度条\n",
    "        for sample in tqdm(split_data, desc=f\"{split_name}\"):\n",
    "            if 'tokens' in sample:\n",
    "                original_tokens = sample['tokens']\n",
    "                total_tokens_before += len(original_tokens)\n",
    "                \n",
    "                # 1. 转换为小写\n",
    "                lowercase_tokens = [token.lower() for token in original_tokens]\n",
    "                \n",
    "                # 2. 只保留tokenizer中存在的tokens\n",
    "                filtered_tokens = [token for token in lowercase_tokens if token in tokenizer.token2idx]\n",
    "                \n",
    "                # 更新统计信息\n",
    "                total_tokens_after += len(filtered_tokens)\n",
    "                removed_tokens_count += (len(original_tokens) - len(filtered_tokens))\n",
    "                \n",
    "                # 检查是否有样本的tokens被完全过滤掉了\n",
    "                if len(filtered_tokens) == 0 and len(original_tokens) > 0:\n",
    "                    samples_with_no_tokens += 1\n",
    "                \n",
    "                # 更新样本中的tokens\n",
    "                sample['tokens'] = filtered_tokens\n",
    "    \n",
    "    # 输出统计信息\n",
    "    print(\"\\n===== Filtering Results =====\")\n",
    "    print(f\"Total tokens before filtering: {total_tokens_before}\")\n",
    "    print(f\"Total tokens after filtering: {total_tokens_after}\")\n",
    "    print(f\"Removed tokens: {removed_tokens_count} ({removed_tokens_count/total_tokens_before*100:.2f}% of original)\")\n",
    "    print(f\"Samples that lost all their tokens: {samples_with_no_tokens}\")\n",
    "    \n",
    "    # 保存处理后的数据\n",
    "    print(f\"Saving filtered data to {output_path}...\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "# 使用函数处理文件\n",
    "input_path = \"/home/ghan/R2Gen/data/iu_xray/annotation_label_with_tokens.json\"\n",
    "output_path = \"/home/ghan/R2Gen/data/iu_xray/annotation_label_with_filtered_tokens.json\"\n",
    "\n",
    "filter_tokens_by_tokenizer(input_path, output_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb8c9751-c86c-4bc3-bdb9-4c0c9ea7d70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4335\n"
     ]
    }
   ],
   "source": [
    "def get_args():\n",
    "    # 使用一个简单的参数对象来替代argparse的解析\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.ann_path = 'data/mimic_cxr/annotation_label_with_tokens.json'\n",
    "            self.threshold = 10\n",
    "            self.dataset_name = 'mimic'\n",
    "            self.max_seq_length = 200\n",
    "            self.seed = 9233\n",
    "            # 添加其他必要的参数\n",
    "    \n",
    "    return Args()\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 主函数\n",
    "def init_tokenizer():\n",
    "    # 获取参数\n",
    "    args = get_args()\n",
    "    \n",
    "    # 设置随机种子\n",
    "    set_seed(args.seed)\n",
    "    \n",
    "    # 创建tokenizer\n",
    "    tokenizer = Tokenizer(args)\n",
    "    return tokenizer\n",
    "\n",
    "# 在Jupyter中运行此代码以获取tokenizer\n",
    "tokenizer = init_tokenizer()\n",
    "# 输出词汇表大小\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "787c960d-3865-405a-a145-e07cbbde1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4335\n"
     ]
    }
   ],
   "source": [
    "# 输出词汇表大小\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da739ef5-10ea-466d-a978-5c48efc3f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/ghan/R2Gen/data/mimic_cxr/annotation_label_with_tokens.json...\n",
      "Processing tokens...\n",
      "Processing train split with 270790 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val split with 2130 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split with 3858 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Filtering Results =====\n",
      "Total tokens before filtering: 6305288\n",
      "Total tokens after filtering: 6294404\n",
      "Removed tokens: 10884 (0.17% of original)\n",
      "Samples that lost all their tokens: 1\n",
      "Saving filtered data to /home/ghan/R2Gen/data/mimic_cxr/annotation_label_with_filtered_tokens.json...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设你已经初始化了tokenizer\n",
    "# tokenizer = init_tokenizer()  # 如果还没有初始化，请先运行之前的代码\n",
    "\n",
    "def filter_tokens_by_tokenizer(input_path, output_path, tokenizer):\n",
    "    print(f\"Loading data from {input_path}...\")\n",
    "    with open(input_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 统计信息\n",
    "    total_tokens_before = 0\n",
    "    total_tokens_after = 0\n",
    "    removed_tokens_count = 0\n",
    "    samples_with_no_tokens = 0\n",
    "    \n",
    "    print(\"Processing tokens...\")\n",
    "    # 处理每个分割\n",
    "    for split_name, split_data in data.items():\n",
    "        print(f\"Processing {split_name} split with {len(split_data)} samples...\")\n",
    "        \n",
    "        # 使用tqdm创建进度条\n",
    "        for sample in tqdm(split_data, desc=f\"{split_name}\"):\n",
    "            if 'tokens' in sample:\n",
    "                original_tokens = sample['tokens']\n",
    "                total_tokens_before += len(original_tokens)\n",
    "                \n",
    "                # 1. 转换为小写\n",
    "                lowercase_tokens = [token.lower() for token in original_tokens]\n",
    "                \n",
    "                # 2. 只保留tokenizer中存在的tokens\n",
    "                filtered_tokens = [token for token in lowercase_tokens if token in tokenizer.token2idx]\n",
    "                \n",
    "                # 更新统计信息\n",
    "                total_tokens_after += len(filtered_tokens)\n",
    "                removed_tokens_count += (len(original_tokens) - len(filtered_tokens))\n",
    "                \n",
    "                # 检查是否有样本的tokens被完全过滤掉了\n",
    "                if len(filtered_tokens) == 0 and len(original_tokens) > 0:\n",
    "                    samples_with_no_tokens += 1\n",
    "                \n",
    "                # 更新样本中的tokens\n",
    "                sample['tokens'] = filtered_tokens\n",
    "    \n",
    "    # 输出统计信息\n",
    "    print(\"\\n===== Filtering Results =====\")\n",
    "    print(f\"Total tokens before filtering: {total_tokens_before}\")\n",
    "    print(f\"Total tokens after filtering: {total_tokens_after}\")\n",
    "    print(f\"Removed tokens: {removed_tokens_count} ({removed_tokens_count/total_tokens_before*100:.2f}% of original)\")\n",
    "    print(f\"Samples that lost all their tokens: {samples_with_no_tokens}\")\n",
    "    \n",
    "    # 保存处理后的数据\n",
    "    print(f\"Saving filtered data to {output_path}...\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "# 使用函数处理文件\n",
    "input_path = \"/home/ghan/R2Gen/data/mimic_cxr/annotation_label_with_tokens.json\"\n",
    "output_path = \"/home/ghan/R2Gen/data/mimic_cxr/annotation_label_with_filtered_tokens.json\"\n",
    "\n",
    "filter_tokens_by_tokenizer(input_path, output_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e72fd0-ae0a-436c-8a97-496621fba310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
